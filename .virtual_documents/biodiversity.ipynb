


# Load required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# Load datasets
observations_data = pd.read_csv('observations.csv')
species_data = pd.read_csv('species_info.csv')

# Display first few rows
observations_data.head()


species_data.head()


# Display summary statistics for the observations dataset
observations_data.describe(include='all')


# Check for missing values in the observations dataset
observations_data.isnull().sum()


species_data.describe(include='all')


species_data.conservation_status.unique()


species_data.conservation_status.fillna('Non-threatened species', inplace=True)
species_data.describe(include='all')


species_data.scientific_name.unique()


ax = sns.histplot(x='conservation_status', data=species_data)
ax.set_xticks(ax.get_xticks())
ax.set_xticklabels(species_data.conservation_status.unique(), rotation=30)
plt.xlabel('Conservation Status')
plt.ylabel('Frequency')
plt.title('Conservation Status Distribution')
plt.show()
plt.close()


species_data.groupby('category').scientific_name.count()


# Calculate the counts and ordenate
category_counts = species_data.groupby('category').scientific_name.count().sort_values(ascending=False)

ax = sns.barplot(x=category_counts.index, y=category_counts.values)
plt.xlabel('Category')
plt.ylabel('Count')
ax.set_xticks(ax.get_xticks())
ax.set_xticklabels(category_counts.index, rotation=30)
plt.title('Category distribution')
plt.show()
plt.close()





park_counts = observations_data.groupby('park_name').scientific_name.count().sort_values(ascending=False)

ax = sns.barplot(x=park_counts.index, y=park_counts.values)
plt.xlabel('Park')
plt.ylabel('Count')
ax.set_xticks(ax.get_xticks())
ax.set_xticklabels(park_counts.index, rotation=30)
plt.title('Park distribution')
plt.show()
plt.close()





observations_data.info()


species_data.info()


species_observations_data = species_data.set_index('scientific_name').join(observations_data.set_index('scientific_name'), how='left')
species_observations_data.head()


species_observations_data.index.nunique()


category_observations = species_observations_data.groupby('category').observations.sum().sort_values(ascending=False)
category_observations.head()


ax = sns.barplot(x=category_observations.index, y=category_observations)
plt.xlabel('Categories')
plt.ylabel('Number of observations')
plt.title('Number of observations by category')
ax.set_xticks(ax.get_xticks())
ax.set_xticklabels(category_observations.index, rotation=30)
plt.show()
plt.close()





park_observations = species_observations_data.groupby('park_name').observations.sum().sort_values(ascending=False)

ax = sns.barplot(x=park_observations.index, y=park_observations)
plt.xlabel('Parks')
plt.ylabel('Number of observations')
plt.title('Number of observations by park')
ax.set_xticks(ax.get_xticks())
ax.set_xticklabels(park_observations.index, rotation=30)
plt.show()
plt.close()





park_category_observations = species_observations_data.groupby(['park_name', 'category']).observations.sum().sort_values(ascending=False).reset_index()
park_observations.head()


g = sns.catplot(x='park_name', y='observations', hue='category', data=park_category_observations, kind='bar', palette='dark', alpha=.6, height=6, aspect=2)
g.despine(left=True)
g.set_axis_labels("Parks", "Number of observations")
g.legend.set_title("Categories")
g.ax.set_title('Number of observations by category in each park')
g.ax.set_xticks(g.ax.get_xticks())
g.ax.set_xticklabels(g.ax.get_xticklabels(), rotation=45, ha='right')
plt.show()
plt.close()


conservation_observations = species_observations_data.groupby(['conservation_status', 'category']).observations.sum().sort_values(ascending=False).reset_index()
conservation_observations.head()


g = sns.catplot(x='category', y='observations', hue='conservation_status', data=conservation_observations, kind='bar', palette='dark', alpha=.6, height=6, aspect=2)
g.despine(left=True)
g.set_axis_labels("Category", "Number of observations")
g.legend.set_title("Conservation STatus")
g.ax.set_title('Number of observations by category according to conservation_status')
g.ax.set_xticks(g.ax.get_xticks())
g.ax.set_xticklabels(g.ax.get_xticklabels(), rotation=45, ha='right')
plt.show()
plt.close()


import scipy.stats as stats
import statsmodels.api as sm

conservation_observations = species_observations_data.groupby(['conservation_status', 'scientific_name']).observations.sum().sort_values(ascending=False).reset_index()
model = sm.OLS.from_formula('observations ~ C(conservation_status)', data=conservation_observations).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
anova_table





kruskal_test = stats.kruskal(
    *[conservation_observations[conservation_observations['conservation_status'] == grupo]['observations'] for grupo in conservation_observations['conservation_status'].unique()]
)
kruskal_test


ss_total = np.sum((conservation_observations['observations'] - np.mean(conservation_observations['observations']))**2)
ss_between = np.sum((conservation_observations.groupby('conservation_status')['observations'].mean() - np.mean(conservation_observations['observations']))**2) * conservation_observations['conservation_status'].value_counts()
eta_squared = ss_between.sum() / ss_total


print("ANOVA:\n", anova_table)
print("\nKruskal-Wallis Teste:\n", kruskal_test)
print("\nEta² (Força da correlação):", eta_squared)





from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error


data = observations_data.merge(species_data, on="scientific_name", how="left")

# Encode the categoric variables
label_encoders = {}
for col in ["park_name", "category", "conservation_status"]:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

# Define X (input) and Y (output)
X = data[["park_name", "category", "conservation_status"]]
y = data["observations"]

# Split the training data into train (80%) an test (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train a Random Forest Regressor Model to estimate the number of observations
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)


# Predict using the Model
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
print(f'Mean Absolute Error (MAE): {mae:.2f}')


y_baseline = [y_train.mean()] * len(y_test)

# Calculate the baseline error
baseline_mae = mean_absolute_error(y_test, y_baseline)

print(f'Baseline MAE {baseline_mae:.2f}')






